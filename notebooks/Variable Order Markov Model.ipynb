{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "=======\n",
    "\n",
    "In this notebook I want to derive the algorithm for constructing a PPM (*Prediction by Partial Match*) model for text content. This is based on\n",
    "\n",
    "*\"On Prediction Using Variable Order Markov Models\"* by Ron Begleiter, Ran El-Yaniv, and Golan Yona in\n",
    "the Journal of Artificial Intelligence Research 22 (2004) 385-421.\n",
    "\n",
    "I have already implemented the code for this on github [here](https://github.com/rpgomez/vomm) but that was for the scenario in which the size of the alphabet is relative small ($\\lessapprox 256$), not the case where the size of the alphabet is large ($\\gtrapprox 2^{15}$) such as the vocabulary of a large collection of documents. \n",
    "\n",
    "In the second case, the amount of memory required to build a PPM model is significantly higher than necessary with a simple, naive approach. So I'm going to document here my approach for conserving on memory in order to implement a reasonable PPM model where the vocabulary are words and a sequence of symbols are a sequence of grammatically correct words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the probability distribution for a categorical variable $X$ with sparse data.\n",
    "\n",
    "* I have a catecorical variable $X$ which takes integer values $1,2, \\ldots, N$ whenre $N \\gg 1$\n",
    "* I have a set of i.i.d. drawn samples of $X$, $s_1,s_2,\\ldots, s_M$ where $M \\ll N$\n",
    "* I want to estimate the probability distribution $Pr(X=i)$ from my samples.\n",
    "\n",
    "What is the appropriate algorithm?\n",
    "\n",
    "### Solution: Simplify the problem\n",
    "\n",
    "I have a random variable $Y$ with 2 possible values $0,1$ with observed counts of $n_0, n_1$. The maximum likelihood estimator (MLE) for $Pr(Y=i)$ is given by \n",
    "\n",
    "$$Pr(Y=i) = \\frac{n_i}{n_0 + n_1}$$\n",
    "\n",
    "To estimate the uncertainty of my estimate I can use Bayesian inference with a [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) [Conjugate Prior](https://en.wikipedia.org/wiki/Conjugate_prior) to estimate $Pr(Y=i)$. I start by using the uninformative prior $Dir(\\alpha= (1,1))$ and then I use the observed counts to update my belief to a posterior distribution:\n",
    "$Dir(\\alpha = (1 + n_0,1 + n_1))$. \n",
    "\n",
    "Using my posterior distribution I can now determine the most likely maximum a posteriori (MAP) distribution (the same as MLE in this case), but can also measure the variance of the possible distributions that explain the observed data.\n",
    "\n",
    "### The more complicated problem\n",
    "I have a categorical value $X$ with $N \\gg 1$, but I only I get to make at most $M \\ll N$ observations. How can I use the simplified problem to estimate $Pr(X=i)$?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "1. Without loss of generality, let $1,\\ldots,R$ be the observed values in the sequence $s_1,\\ldots, s_M$.\n",
    "2. Let $Y$ denote a random variable which takes on values in the range $1,\\ldots, R$.\n",
    "3. Let $Z$ denote a random variable which takes on values in the range $R+1,\\ldots, N$.\n",
    "4. We can interpret $X$ a mixture of the 2 random variables $Y,Z$ as follows:\n",
    "\\begin{align}\n",
    "Pr(X = i) &= Pr(X=i, Y \\text{ is selected}) + Pr(X=i, Z\\text{ is selected}) \\\\\n",
    "&= Pr(X=i| Y \\text{ is selected}) Pr(Y \\text{ is selected}) + Pr(X=i| Z\\text{ is selected})Pr( Z\\text{ is selected}) \\\\\n",
    "&= Pr(X=i| Y \\text{ is selected}) \\omega + Pr(X=i| Z\\text{ is selected})(1-\\omega) \\\\\n",
    "&= Pr(Y=i| Y \\text{ is selected}) \\omega + Pr(Z=i| Z\\text{ is selected})(1-\\omega) \\\\\n",
    "&= Pr(Y=i) \\omega + Pr(Z=i)(1-\\omega) \\\\\n",
    "\\end{align}\n",
    "where $0 \\le \\omega \\le 1$ is the mixing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate $\\omega$ using the simplified problem:\n",
    "\n",
    "$$(\\omega, 1-\\omega) \\sim Dir\\left(\\alpha = (1 + n_Y, 1 + n_Z)\\right) = Dir\\left(\\alpha = (1 + M, 1)\\right)$$\n",
    "\n",
    "whose mean value is given $ \\omega = (1 + M)/(2 + M)$. We should not use the MAP/MLE value:\n",
    "$\\omega = M/M = 1$ as that implies observations of $Z$ never occur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the probability distribution for $Y$ using the simplified problem.\n",
    "\n",
    "Let $c_i, i = 1, \\ldots, R$ be the observed frequency counts of $X=i$. Then $$Y \\sim Dir(\\alpha = ( 1+c_1, \\ldots, 1 + c_R))$$\n",
    "\n",
    "We can choose to use the mean value distribution for $Y$:\n",
    "\n",
    "$$Pr(Y=i) = \\frac{1 + c_i}{R + \\sum_j c_j}$$\n",
    "\n",
    "or the MLE/MAP:\n",
    "\n",
    "$$Pr(Y=i) = \\frac{c_i}{\\sum_j c_j}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we never get to observe $Z$ we only have the uninformative prior $Z \\sim Dir(\\alpha = (1,1,\\ldots,1))$ whose mean distribution for $Z$ is $Pr(Z = i) = 1/(N - R)$. Because $1-\\omega \\ll \\omega$ using the mean distribution should not cause significant errors in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $Pr(word| context)$\n",
    "\n",
    "From above we can now estimate for a given observed context of size $d$ the probability distribution of the word that follows. \n",
    "\n",
    "In order to conserve memory \n",
    "\n",
    "* we first translate all words into numbers, $1, 2 , \\ldots, W$ where $W$ is the size of the vocabulary\n",
    "* create a dictionary *Counts* with keys all observed contexts of size $d$ and values another dictionary where\n",
    "    * the value is a dictionary that has (key, value) pairs (word, # of occurrences after the specific context), i.e.\n",
    "     Counts(c)(w) = # of times word *w* was observed to follow context *c*\n",
    "* create a dictionary *Probabilities* \n",
    "    * with keys all observed contexts of size $d$ \n",
    "    * and for each key a dictionary with (w, p) pair where \n",
    "        * *w* is a word observed to follow the context \n",
    "        * and p is the probability of *w* occurring given by \n",
    "        $$\\omega_{c}\\times c_w/\\sum_v c_v$$ where \n",
    "            * $\\omega_c$ is the mixing factor determined for the given context $c$, \n",
    "            * and $c_v$ is the number of times word $v$ was observed to follow context $c$.\n",
    "        * There is 1 additional (w,p) pair with key  *none* and value $(1-\\omega_c)/(W-R_c)$ where\n",
    "            * $W-R_c$ is the number of vocabulary words that were **not** observed to follow context $c$.\n",
    "        \n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_counts_dictionary(observed_sequence,d=4,debug=False):\n",
    "    \"\"\"Constructs the dictionary of observed counts of words following \n",
    "    contexts at most d words long. \n",
    "    \n",
    "    observed_sequence should be a sequence of type integer of non-negative integers.\n",
    "    Each integer corresponds to some word in the vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not debug:\n",
    "        def use_me(x):\n",
    "            return x\n",
    "    else:\n",
    "        use_me = tqdm\n",
    "            \n",
    "    counts = defaultdict(Counter)\n",
    "    N = len(observed_sequence)\n",
    "    \n",
    "    for t in use_me(range(N)):\n",
    "        obs = observed_sequence[t]\n",
    "        for l in range(0,d+1):\n",
    "            if t - l < 0:\n",
    "                break\n",
    "            context = observed_sequence[t-l:t]\n",
    "            counts[context][obs] +=1\n",
    "    \n",
    "    return counts\n",
    "            \n",
    "def construct_prob_dictionary(counts_dictionary,vocabulary_size,debug=False):\n",
    "    \"\"\"Computes the memory efficient dictionary word| context probabilities\"\"\"\n",
    "\n",
    "    V = vocabulary_size\n",
    "    \n",
    "    if not debug:\n",
    "        def use_me(x):\n",
    "            return x\n",
    "    else:\n",
    "        use_me = tqdm\n",
    "            \n",
    "    probs_dictionary = dict()\n",
    "    for context in use_me(counts_dictionary):\n",
    "        local_list = counts_dictionary[context]\n",
    "        total_count = sum(list(local_list.values()))\n",
    "        \n",
    "        R = len(local_list)\n",
    "        if R == V:\n",
    "            omega = 1.0\n",
    "        else:\n",
    "            omega = (1+ total_count)/(2 + total_count)\n",
    "            \n",
    "        probs_dictionary[context] = defaultdict(float)\n",
    "        \n",
    "        local_probs = defaultdict(float)\n",
    "        for word in local_list:\n",
    "            c_i = local_list[word]\n",
    "            pr = c_i/total_count * omega\n",
    "            local_probs[word] = pr\n",
    "        \n",
    "        if R < V:\n",
    "            local_probs[None] = (1.0 - omega)/(V-R)\n",
    "        else:\n",
    "            local_probs[None] = 0.0\n",
    "        \n",
    "        probs_dictionary[context] = local_probs\n",
    "    \n",
    "    return probs_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_pdf_dict(probs_dictionary,debug=False):\n",
    "    \"\"\"Computes the log likelihood dictionary\n",
    "    log (Pr(symbol|context)) from the probs_dictionary\"\"\"\n",
    "    \n",
    "    if not debug:\n",
    "        def use_me(x):\n",
    "            return x\n",
    "    else:\n",
    "        use_me = tqdm\n",
    "    \n",
    "    log_pdf_dict = {}\n",
    "    for context in use_me(probs_dictionary):\n",
    "        local_pdf_list = probs_dictionary[context]\n",
    "        local_log_pdf_list = {}\n",
    "        for asymbol in local_pdf_list:\n",
    "            local_log_pdf_list[asymbol] = np.log(local_pdf_list[asymbol])\n",
    "            \n",
    "        log_pdf_dict[context] = local_log_pdf_list\n",
    "    \n",
    "    return log_pdf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPM_words:\n",
    "    \"\"\"This class is designed to be memory efficient in the case that the size of the vocabulary is large\n",
    "    (>= 2**15 for example).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Not much to do here. \"\"\"\n",
    "\n",
    "    def generate_fast_lookup(self,verbose=False):\n",
    "        \"\"\"Takes the pdf_dict dictionary and computes a faster lookup\n",
    "        dictionary mapping suffix s to its longer contexts xs.  I need\n",
    "        this to speed up computing the probability of logpdf for an\n",
    "        observed sequence\n",
    "        \"\"\"\n",
    "\n",
    "        if not verbose:\n",
    "            def use_me(x,desc=None):\n",
    "                return x\n",
    "        else:\n",
    "            use_me = tqdm\n",
    "\n",
    "        # I want to create a fast look up of context s -> xs\n",
    "        # So scoring a sequence is faster.\n",
    "        \n",
    "        context_lengths = [0]*(self.d+1)\n",
    "        for context in self.pdf_dict:\n",
    "            try:\n",
    "                l = len(context)\n",
    "            except:\n",
    "                l  = 0\n",
    "                \n",
    "            context_lengths[l] += 1\n",
    "        context_by_length  = dict([(k,[0]*context_lengths[k]) for k in range(self.d+1) ])\n",
    "\n",
    "        indices = [0]*(self.d+1)\n",
    "        for x in use_me(self.pdf_dict.keys(),desc='populating context_by_length'):\n",
    "            try:\n",
    "                l = len(x)\n",
    "            except:\n",
    "                l = 0\n",
    "            t = indices[l]\n",
    "            context_by_length[l][t] = x\n",
    "            indices[l] += 1\n",
    "            \n",
    "\n",
    "        # Now lets generate a dictionary look up context s -> possible context xs.\n",
    "        self.context_child = {}\n",
    "\n",
    "        for k in range(self.d):\n",
    "            for x in use_me(context_by_length[k],desc='context_child %d' %k):\n",
    "                self.context_child[x] = [ y for y in context_by_length[k+1] if y[1:] == x ]\n",
    "\n",
    "        for x in context_by_length[self.d]:\n",
    "            self.context_child[x] = []\n",
    "\n",
    "    def fit(self,training_data, d=4, alphabet_size = None,verbose=False):\n",
    "        \"\"\"\n",
    "        This is the method to call to fit the model to the data.\n",
    "        training_data should be a sequence of symbols represented by\n",
    "        integers 0 <= x < alphabet size.\n",
    "\n",
    "        d specifies the largest context sequence we're willing to consider.\n",
    "\n",
    "        alphabet_size specifies the number of distinct symbols that\n",
    "        can be possibly observed. If not specified, the alphabet_size\n",
    "        will be inferred from the training data.\n",
    "        \"\"\"\n",
    "\n",
    "        if alphabet_size == None:\n",
    "            alphabet_size = max(training_data) + 1\n",
    "\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.d = d\n",
    "\n",
    "        counts = construct_counts_dictionary(training_data,d=d,debug=verbose)\n",
    "        self.pdf_dict = construct_prob_dictionary(counts,alphabet_size,debug=verbose)\n",
    "\n",
    "        self.logpdf_dict = generate_log_pdf_dict(self.pdf_dict,debug=verbose)\n",
    "\n",
    "        # For faster look up  when computing logpdf(observed data).\n",
    "        self.generate_fast_lookup(verbose=verbose)\n",
    "\n",
    "        return\n",
    "\n",
    "    def logpdf(self,observed_data):\n",
    "        \"\"\"Call this method after using fitting the model to compute the log of\n",
    "        the probability of an observed sequence of data.\n",
    "\n",
    "        observed_data should be a sequence of symbols represented by\n",
    "        integers 0 <= x < alphabet_size. \"\"\"\n",
    "\n",
    "        temp = tuple(observed_data)\n",
    "        # start with the null context and work my way forward.\n",
    "\n",
    "        logprob = 0.0\n",
    "        for t in range(len(temp)):\n",
    "            chunk = temp[max(t-self.d,0):t]\n",
    "            sigma = temp[t]\n",
    "            context = find_largest_context(chunk,self.context_child,self.d)\n",
    "            if sigma in self.logpdf_dict[context]:\n",
    "                logprob += self.logpdf_dict[context][sigma]\n",
    "            else:\n",
    "                logprob += self.logpdf_dict[context][None] # Didn't see this symbol with the context\n",
    "        return logprob\n",
    "\n",
    "    def generate_data(self,prefix=None, length=200,verbose=False):\n",
    "        \"\"\"Generates data from the fitted model.\n",
    "\n",
    "        The length parameter determines how many symbols to generate.\n",
    "\n",
    "        prefix is an optional sequence of symbols to be appended to,\n",
    "        in other words, the prefix sequence is treated as a set of\n",
    "        symbols that were previously \"generated\" that are going to be\n",
    "        appended by an additional \"length\" number of symbols.\n",
    "\n",
    "        The default value of None indicates that no such prefix\n",
    "        exists. We're going to be generating symbols starting from the\n",
    "        null context.\n",
    "\n",
    "        It returns the generated data as an array of symbols\n",
    "        represented as integers 0 <=x < alphabet_size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not verbose:\n",
    "            def use_me(x):\n",
    "                return x\n",
    "        else:\n",
    "            use_me = tqdm\n",
    "\n",
    "        if prefix != None:\n",
    "            new_data = np.zeros(len(prefix) + length,dtype=np.int)\n",
    "            new_data[:len(prefix)] = prefix\n",
    "            start = len(prefix)\n",
    "        else:\n",
    "            new_data = np.zeros(length,dtype=np.int)\n",
    "            start = 0\n",
    "\n",
    "        scratch_pdf = np.zeros(self.alphabet_size)\n",
    "        for t in use_me(range(start,len(new_data))):\n",
    "            chunk = tuple(new_data[max(t-self.d,0):t])\n",
    "            context = find_largest_context(chunk,self.context_child,self.d)\n",
    "            \n",
    "            scratch_pdf[:] = self.pdf_dict[context][None] # the symbols we didn't see\n",
    "            for symbol in self.pdf_dict[context]:\n",
    "                if symbol  == None:\n",
    "                    continue\n",
    "                scratch_pdf[symbol] = self.pdf_dict[context][symbol]\n",
    "                \n",
    "            new_symbol = np.random.choice(self.alphabet_size,p=scratch_pdf)\n",
    "            new_data[t] = new_symbol\n",
    "\n",
    "        return new_data[start:]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Implements a string representation to return the parameters of this model. \"\"\"\n",
    "\n",
    "        return \"\\n\".join([\"alphabet size: %d\" % self.alphabet_size,\n",
    "                          \"context length d: %d\" % self.d,\n",
    "                          \"Size of model: %d\" % len(self.pdf_dict)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moby Dick\n",
    "I'm going to create a PPM model for the content of Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.tokenize.word_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = open('mobydick.txt','r').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = string.ascii_letters\n",
    "def filter_nonletters(aletter):\n",
    " if aletter in filter:\n",
    "    return aletter\n",
    " else:\n",
    "    return \" \"\n",
    "new_content = \"\".join([filter_nonletters(x) for x in tqdm(content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.tokenize.word_tokenize(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(words)\n",
    "print(\"Size of vocabulary: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_words = list(vocabulary)\n",
    "words_to_numbers = {}\n",
    "for t,word in enumerate(numbers_to_words):\n",
    "    words_to_numbers[word] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sequence = tuple([words_to_numbers[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sequence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = PPM_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(obs_sequence,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
